{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Get or create Workspace and create AML Compute cluster #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml\n",
    "from azureml.core import Run\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.experiment import Experiment\n",
    "\n",
    "print(\"SDK Version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the Subscription ID of your existing Azure subscription\n",
    "subscription_id = \"xxx-xxx-xxx\"\n",
    "\n",
    "# Provide values for the Resource Group and Workspace that will be created\n",
    "resource_group = \"service-labs\"\n",
    "workspace_name = \"service-labs-ws\"\n",
    "workspace_region = 'eastus'  # eastus, westcentralus, southeastasia, australiaeast, westeurope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By using the exist_ok param, if the worskpace already exists we get a reference to the existing workspace\n",
    "ws = Workspace.create(\n",
    "    name = workspace_name,\n",
    "    subscription_id = subscription_id,\n",
    "    resource_group = resource_group, \n",
    "    location = workspace_region,\n",
    "    exist_ok = True)\n",
    "\n",
    "print(\"Workspace Provisioning complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create AML CPU based Compute Cluster\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cluster_name = \"amlcompute-ad\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_NC6',\n",
    "                                                           min_nodes=1, max_nodes=1)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# Use the 'status' property to get a detailed status for the current AmlCompute. \n",
    "print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Create training script #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "import numpy as np\n",
    "import timeit\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model as KModel\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import azureml\n",
    "from azureml.core import Run\n",
    "from azureml.core.model import Model\n",
    "import pickle\n",
    "\n",
    "print(\"SDK Version:\", azureml.core.VERSION)\n",
    "\n",
    "# We use Fashion mnist dataset\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "# We download and load the data\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "print('Fashion MNIST dataset loaded!')\n",
    "\n",
    "# Build the encoder\n",
    "input_img = Input(shape=(28, 28, 1))\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded_feature_vector = MaxPooling2D((2, 2), padding='same', name='feature_vector')(x)\n",
    "\n",
    "# at this point the representation is (4, 4, 8) i.e. 128-dimensional compressed feature vector\n",
    "\n",
    "# Build the decoder\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded_feature_vector)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded_output = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "# The first model is autoencoder model, it takes the input image and results in a decoded image\n",
    "autoencoder_model = KModel(input_img, decoded_output)\n",
    "# Compile the first model\n",
    "autoencoder_model.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "# The second NN model is only a half of the first model\n",
    "# it take the input image and gives the encoded vector as output\n",
    "encoder_model = KModel(inputs=autoencoder_model.input, \n",
    "                      outputs=autoencoder_model.get_layer('feature_vector').output) #output from feature vector\n",
    "# Compile the second model\n",
    "encoder_model.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "print('')\n",
    "print(autoencoder_model.summary())\n",
    "print('')\n",
    "\n",
    "# We need to scale the image from [0-255] to [0-1] for better performance of activation functions\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "\n",
    "# We train the NN in batches (groups of images), so we reshape the dataset\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
    "\n",
    "print(\"Train dataset size is {0}\".format(x_train.shape))\n",
    "print(\"Test dataset size is {0}\".format(x_test.shape))\n",
    "\n",
    "print(\"Model training starting...\")\n",
    "start_time = timeit.default_timer()\n",
    "# It takes several minutes to train this neural network, depending on the configuration of your cluster.\n",
    "history=autoencoder_model.fit(x=x_train, y=x_train, epochs=10, batch_size=128, \n",
    "                                       shuffle=True, validation_data=(x_test, x_test), verbose=1)\n",
    "elapsed_time = timeit.default_timer() - start_time\n",
    "print(\"Model training completed.\")\n",
    "print('Elapsed time (min): ', round(elapsed_time/60.0,0))\n",
    "\n",
    "os.makedirs('./outputs', exist_ok=True)\n",
    "\n",
    "# save the models\n",
    "autoencoder_model.save(os.path.join('./outputs', 'autoencoder_model.h5'))\n",
    "encoder_model.save(os.path.join('./outputs', 'encoder_model.h5'))\n",
    "\n",
    "# save training history\n",
    "with open(os.path.join('./outputs', 'history.txt'), 'w') as f:\n",
    "    f.write(str(history.history))\n",
    "\n",
    "print(\"Models saved in ./outputs folder\")\n",
    "print(\"Saving model files completed.\")\n",
    "\n",
    "# Register the Models\n",
    "run = Run.get_context()\n",
    "\n",
    "os.chdir(\"./outputs\")\n",
    "\n",
    "model_path = 'autoencoder_model.h5'\n",
    "model_name = 'fashion_autoencoder'\n",
    "model_description = 'Autoencoder network for Fashion-MNIST dataset.'\n",
    "model = Model.register(\n",
    "    model_path=model_path,  # this points to a local file\n",
    "    model_name=model_name,  # this is the name the model is registered as\n",
    "    tags={\"type\": \"autoencoder\", \"run_id\": run.id},\n",
    "    description=model_description,\n",
    "    workspace=run.experiment.workspace\n",
    ")\n",
    "\n",
    "os.chdir(\"..\")\n",
    "\n",
    "print(\"Model registered: {} \\nModel Description: {} \\nModel Version: {}\".format(model.name, \n",
    "                                                                                model.description, \n",
    "                                                                                model.version))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Remotely Train the Autoencoder Network using the AML Compute #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.dnn import TensorFlow\n",
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "keras_est = TensorFlow(source_directory='.',\n",
    "                       compute_target=compute_target,\n",
    "                       entry_script='train.py', \n",
    "                       conda_packages=['numpy==1.16.4'], \n",
    "                       pip_packages=['keras==2.3.1'], \n",
    "                       framework_version='2.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'aml-service-lab05'\n",
    "experiment = Experiment(ws, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(keras_est, tags = {\"type\": \"autoencoder\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor the run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the azureml Jupyter widget, you can monitor the training run. This will approximately take around 5-10 minutes to complete. Once the training is completed you can then download the trained models locally by running the **Download the trained models** cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, if you are using **Visual Studio Code**, the RunDetails widget is currently not supported. Uncomment the line below and run the cell to monitor and wait for the experiment run to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Download the trained models #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an output folder in the current directory\n",
    "os.makedirs('./outputs', exist_ok=True)\n",
    "\n",
    "for f in run.get_file_names():\n",
    "    if f.startswith('outputs'):\n",
    "        output_file_path = os.path.join('./outputs', f.split('/')[-1])\n",
    "        print('Downloading from {} to {} ...'.format(f, output_file_path))\n",
    "        run.download_file(name=f, output_file_path=output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls './outputs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 - Review registered model #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model_list = Model.list(ws)\n",
    "\n",
    "latest_model = sorted(model_list, reverse=True, key = lambda x: x.created_time)[0]\n",
    "latest_model_name = latest_model.name\n",
    "latest_model_path = latest_model.get_model_path(latest_model_name, _workspace=ws)\n",
    "latest_model_description = latest_model.description\n",
    "latest_model_version = latest_model.version\n",
    "latest_model_run_id = latest_model.tags.get(\"run_id\")\n",
    "\n",
    "print('Model name: ', latest_model_name)\n",
    "print('Model path: ', latest_model_path)\n",
    "print('Model description: ', latest_model_description)\n",
    "print('Model version: ', latest_model_version)\n",
    "print('Training run id: ', latest_model_run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
